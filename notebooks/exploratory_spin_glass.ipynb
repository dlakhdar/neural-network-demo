{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#TODO: produce help strings \n",
    "#TODO: better type annotation\n",
    "#TODO: implement different initializations \n",
    "#TODO: decouple input output from init method in class\n",
    "#TODO: implement AD with jax \n",
    "#TODO: eliminate use of lists , optimize with jax and numba  \n",
    "#TODO: implement momentum, adam \n",
    "#TODO: implement convolutional network \n",
    "\n",
    "fire = 1\n",
    "output_length = 10 \n",
    "\n",
    "def magnitude(x: np.array) -> (int | float):\n",
    "    return np.sqrt(np.sum(x**2))\n",
    "    \n",
    "def max_normalize(data):\n",
    "    return data / np.max(data)\n",
    "\n",
    "def gaussian_normalize(data):\n",
    "    return (data - np.mean(data, axis=0)) / (np.std(data, axis=0) + 1e-8)    \n",
    "# @nb.jit()\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative_relu(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def sigmoid(z : (int | float)) -> (int | float, np.array):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of a given input.\n",
    "\n",
    "    The sigmoid function is commonly used as an activation function in neural networks.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : int or float\n",
    "        The input value for which the sigmoid function will be computed.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int or float\n",
    "        The computed sigmoid value of the input, in the range (0, 1).\n",
    "\n",
    "    Note:\n",
    "    -----\n",
    "    The function expects scalar inputs. For vectorized inputs (e.g., NumPy arrays),\n",
    "    consider extending this function or directly using vectorized NumPy operations.\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def derivative_sigmoid(z : (int | float )) -> (int | float, \"np.array\")  : \n",
    "    \"\"\"\n",
    "    Computes the derivative of of a given input.\n",
    "\n",
    "    The sigmoid function is commonly used as an activation function in neural networks.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : int or float\n",
    "        The input value for which the sigmoid function will be computed.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int or float\n",
    "        The computed sigmoid value of the input, in the range (0, 1).\n",
    "\n",
    "    Note:\n",
    "    -----\n",
    "    The function expects scalar inputs. For vectorized inputs (e.g., NumPy arrays),\n",
    "    consider extending this function or directly using vectorized NumPy operations.\n",
    "    \"\"\"\n",
    "    sig = sigmoid\n",
    "    return sig(z) * (1-sig(z))\n",
    "\n",
    "\n",
    "def mse_grad(a:np.array, y:np.array) -> np.array:\n",
    "    return (a-y)\n",
    "\n",
    "\n",
    "def hot_encode(x: np.array, output_length:(int | float)) -> np.array:   \n",
    "    \"\"\"\n",
    "    Converts an array of integer indices into one-hot encoded vectors.\n",
    "\n",
    "    Parameters:\n",
    "        x (np.array): Array of integer indices.\n",
    "        output_length (int): Length of the one-hot encoded vectors.\n",
    "\n",
    "    Returns:\n",
    "        np.array: A 2D array where each row is a one-hot encoded vector corresponding to the input indices.\n",
    "    \"\"\"   \n",
    "    tmp = []\n",
    "    for index in x:\n",
    "        x_vec = np.zeros(output_length)\n",
    "        x_vec[int(index)] = fire\n",
    "        tmp.append(x_vec.reshape((output_length)))\n",
    "    return np.array(tmp)\n",
    "\n",
    "def feedforward(x: np.array,σ: callable,n: (int | float), W: \"NeuralNetwork.weights\",b:\"NeuralNetwork.bias\")-> np.array:\n",
    "    \"\"\"\n",
    "    Perform a feedforward computation in a neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "        The input array to the neural network. Typically a vector or batch of vectors.\n",
    "    σ : callable\n",
    "        The activation function applied element-wise at each layer (e.g., ReLU, sigmoid, or tanh).\n",
    "    n : int | float\n",
    "        The number of layers in the neural network. Assumes layers are indexed from 0 to n-1.\n",
    "    W : NeuralNetwork.weights\n",
    "        A list or array of weight matrices, where `W[l]` is the weight matrix for layer `l`.\n",
    "        Each matrix should have dimensions suitable for the connections between layers.\n",
    "    b : NeuralNetwork.bias\n",
    "        A list or array of bias vectors, where `b[l]` is the bias vector for layer `l`.\n",
    "        Each vector should have dimensions matching the output size of the respective layer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The output of the neural network after applying all layers.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function assumes that the number of layers (`n`) matches the length of `W` and `b`.\n",
    "    - The activation function (`σ`) is applied after the affine transformation at each layer:\n",
    "      `activation = σ(W[l] @ activation + b[l])`.\n",
    "    - `x` is treated as the initial activation for layer 0.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import numpy as np\n",
    "    >>> def relu(x):\n",
    "    ...     return np.maximum(0, x)\n",
    "    ...\n",
    "    >>> x = np.array([1, 2])\n",
    "    >>> W = [np.array([[0.1, 0.2], [0.3, 0.4]]), np.array([[0.5, 0.6]])]\n",
    "    >>> b = [np.array([0.1, 0.2]), np.array([0.3])]\n",
    "    >>> feedforward(x, relu, 2, W, b)\n",
    "    array([0.77])  # Example output\n",
    "    \"\"\"\n",
    "    \n",
    "    activation = σ(W[0] @ x + b[0])\n",
    "    for l in range(1, n):\n",
    "        activation = σ(W[l] @ activation + b[l])\n",
    "\n",
    "    return activation\n",
    "\n",
    "def store_intermediate(feedforward):\n",
    "    \"\"\"\n",
    "    Decorator to modify the feedforward method to store intermediate activations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feedforward_func : function\n",
    "        The original feedforward function to be wrapped.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    function\n",
    "        A new function that stores intermediate activations.\n",
    "    \"\"\"\n",
    "    @wraps(feedforward)\n",
    "    def wrapper(x, σ, n, W, b):\n",
    "        # List to store intermediate activations\n",
    "        A = []\n",
    "        Z = []\n",
    "        # Perform the forward pass, storing intermediate activations at each layer\n",
    "        z0 = W[0] @ x + b[0]\n",
    "        a0 = σ(z0)\n",
    "        a = a0\n",
    "        Z.append(z0);A.append(a0)  # Store the first activation\n",
    "        for l in range(1, n):\n",
    "            z = W[l] @ a + b[l]\n",
    "            a = σ(z)\n",
    "            Z.append(z);A.append(a)  # Store the activation at each layer\n",
    "        \n",
    "        # Call the original feedforward function (return final result)\n",
    "        result = a\n",
    "        \n",
    "        # Return the result and the list of intermediate activations\n",
    "        return result, A, Z\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def prepare_data(dataset = 'mnist', normalize_scheme=max_normalize):     \n",
    "        \"\"\"\n",
    "    Prepares and preprocesses a dataset for training and testing.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (str): Name of the dataset to load from tf.keras.datasets (default: 'mnist').\n",
    "        normalize_scheme (function): Function to normalize the dataset (default: max_normalize).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Preprocessed training and testing data:\n",
    "            - x_train (np.array): Flattened and normalized training input data.\n",
    "            - y_train (np.array): One-hot encoded training labels.\n",
    "            - x_test (np.array): Flattened and normalized testing input data.\n",
    "            - y_test (np.array): One-hot encoded testing labels.\n",
    "        \"\"\"\n",
    "            # Dynamically get the dataset\n",
    "        try:\n",
    "            dataset_module = getattr(tf.keras.datasets, dataset)\n",
    "        except AttributeError:\n",
    "            raise ValueError(f\"Dataset '{dataset}' not found in tf.keras.datasets\")\n",
    "        (x_train, y_train), (x_test, y_test) = dataset_module.load_data()\n",
    "        x_train, y_train = np.array(x_train, dtype=float) , np.array(y_train, dtype=float)\n",
    "\n",
    "        # Take n number of 28*28 matrices and convert them to 784 vectors \n",
    "        (r, m, n), (rt, mt, nt) = x_train.shape, x_test.shape\n",
    "        dim_x, dim_xt = (r, m*n), (rt, mt*nt)\n",
    "        x_train, x_test = x_train.reshape(dim_x), x_test.reshape(dim_xt)\n",
    "       \n",
    "        y_train, y_test = (hot_encode(y_train, output_length),\n",
    "                           hot_encode(y_test, output_length))\n",
    "\n",
    "        # normalize datasets \n",
    "        # x_train = (x_train - np.mean(x_train, axis=0)) / (np.std(x_train, axis=0) + 1e-8)\n",
    "        # x_test = (x_test - np.mean(x_test, axis=0)) / (np.std(x_test, axis=0) + 1e-8)\n",
    "        x_train,  x_test = map(normalize_scheme, [x_train, x_test])\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    A class for constructing and training a fully connected neural network.\n",
    "\n",
    "    Attributes:\n",
    "        input (np.array): Input training data.\n",
    "        output (np.array): Expected output labels (e.g., one-hot encoded).\n",
    "        hidden_layer_n (int): Number of hidden layers in the network.\n",
    "        layer_n (int): Total number of layers (input + hidden + output).\n",
    "        layer_sizes (np.array): List of sizes for each layer in the network.\n",
    "        bias (list): List of bias vectors for each layer.\n",
    "        weights (list): List of weight matrices connecting the layers.\n",
    "        activation_f (callable): Activation function (default: sigmoid).\n",
    "        activation_df (callable): Derivative of the activation function.\n",
    "        cost_function (callable): Cost function for training (if provided).\n",
    "\n",
    "    Methods:\n",
    "        train(minibatch=True, minibatch_pool=10, iterations=100, η=1e-6) -> 'NeuralNetwork':\n",
    "            Trains the neural network using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "        input (np.array): Input training data, where each row is a training example.\n",
    "        output (np.array): Output labels for the training data.\n",
    "        hidden_layer (int): Number of hidden layers in the network.\n",
    "        layer_sizes (list[int | float]): List of hidden layer sizes (default: [10]).\n",
    "        activation_function (callable): Activation function for all layers (default: sigmoid).\n",
    "        activation_derivative (callable): Derivative of the activation function (default: derivative_sigmoid).\n",
    "        cost (callable): Cost function to minimize during training (optional).\n",
    "        dtype (type): Data type for the network parameters (default: float).\n",
    "\n",
    "    Train Method Parameters:\n",
    "        minibatch (bool): Whether to use mini-batch gradient descent (default: True).\n",
    "        minibatch_pool (int | float): Number of samples per mini-batch (default: 10).\n",
    "        iterations (int | float): Number of training iterations (default: 100).\n",
    "        η (int | float): Learning rate for gradient descent (default: 1e-6).\n",
    "\n",
    "    Returns:\n",
    "        NeuralNetwork: The trained neural network object.\n",
    "\n",
    "    Example:\n",
    "        nn = NeuralNetwork(input=x_train, \n",
    "                           output=y_train, \n",
    "                           hidden_layer=2, \n",
    "                           layer_sizes=[64, 32], \n",
    "                           activation_function=sigmoid, \n",
    "                           activation_derivative=derivative_sigmoid)\n",
    "        nn.train(minibatch=True, minibatch_pool=32, iterations=1000, η=0.01)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 layer_sizes: (list[int] | list[float]) = [10] ,\n",
    "                 activation_function: callable = sigmoid, \n",
    "                 activation_derivative: callable = derivative_sigmoid,\n",
    "                 cost_function: callable = None, \n",
    "                 cost_grad: callable = mse_grad) -> 'NeuralNetwork':\n",
    "        \n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.layer_n = len(self.layer_sizes)\n",
    "        self.hidden_layer_n = len(self.layer_sizes)-2\n",
    "        self.bias = [np.random.randn(self.layer_sizes[i])\n",
    "                     for i in range(1, self.hidden_layer_n+2)] \n",
    "    \n",
    "        self.weights = [np.random.randn(self.layer_sizes[i], \n",
    "                                          self.layer_sizes[i-1]) * np.sqrt(1/self.layer_sizes[i-1])\n",
    "                        for i in range(1, self.hidden_layer_n+2)]\n",
    "        \n",
    "        \n",
    "        self.activation_f = activation_function\n",
    "        self.activation_df= activation_derivative\n",
    "        self.cost = cost_function\n",
    "        self.cost_grad = cost_grad\n",
    "\n",
    "\n",
    "    def train(self,\n",
    "              input, \n",
    "              output, \n",
    "              minibatch: bool = True,\n",
    "              minibatch_pool : (int | float) = 10,\n",
    "              iterations: (int | float) = 100,\n",
    "              η: (int | float) = 1e-6) -> None:\n",
    "        \"\"\"\n",
    "    Trains the neural network using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "        minibatch (bool): Whether to use mini-batch gradient descent (default: True).\n",
    "        minibatch_pool (int | float): Size of the mini-batch for training (default: 10).\n",
    "        iterations (int | float): Number of training iterations (default: 100).\n",
    "        η (int | float): Learning rate for gradient descent (default: 1e-6).\n",
    "\n",
    "    Returns:\n",
    "        NeuralNetwork: The trained neural network object.\n",
    "\n",
    "    Description:\n",
    "        - Implements forward propagation for each input to compute activations.\n",
    "        - Performs backpropagation to compute gradients for weights and biases.\n",
    "        - Updates weights and biases using gradient descent.\n",
    "        - Supports mini-batch gradient descent if `minibatch` is set to True.\n",
    "\n",
    "    Example:\n",
    "        nn.train(minibatch=True, minibatch_pool=32, iterations=1000, η=0.01)\n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "\n",
    "            if minibatch:\n",
    "                indexes = np.random.choice(input.shape[0], size=minibatch_pool)\n",
    "                X, Y = input[indexes], output[indexes]\n",
    "            else: \n",
    "                X, Y = input, output\n",
    "\n",
    "\n",
    "            a_errors = [np.zeros(matrix.shape) for matrix in self.weights]\n",
    "\n",
    "            b_errors = [np.zeros(vector.shape) for vector in self.bias]\n",
    "\n",
    "            # iterate for each set of x and y\n",
    "            # find zs and as (pre-act and activation)\n",
    "            for x, y in zip(X, Y): \n",
    "                # print(\"x id:\",id(x))\n",
    "                z0 = self.weights[0] @ x + self.bias[0]\n",
    "                zs = [z0]\n",
    "                a0 = self.activation_f(z0)\n",
    "                activations = [a0]\n",
    "                for l in range(1, self.layer_n-1,1):\n",
    "                    # print(\"layers:\",l,l-1)\n",
    "                    zl = self.weights[l] @ activations[l-1] + self.bias[l]\n",
    "                    activation = self.activation_f(zl)\n",
    "                    # print(\"activation:\", activation)\n",
    "                    zs.append(zl)\n",
    "                    activations.append(activation)  \n",
    "\n",
    "                z_output = zs[-1]\n",
    "                a_output = activations[-1]\n",
    "                output_error = self.cost_grad(a_output,y) * self.activation_df(z_output)\n",
    "                errors = [output_error]\n",
    "                for l in range(self.hidden_layer_n, 0, -1):\n",
    "                    error = self.weights[l].T @ errors[-1] * self.activation_df(zs[l-1])\n",
    "                    errors.append(error)\n",
    "\n",
    "                errors.reverse()\n",
    "                # compute sum of error\n",
    "                for l in range(0,self.hidden_layer_n+1,1):\n",
    "                    # a_errors[l] += errors[l]@activations[l].T\n",
    "                    a_errors[l] += np.outer(errors[l], activations[l-1]  if l > 0 else x)\n",
    "                    # print(a_errors)\n",
    "                    b_errors[l] += errors[l] \n",
    "                        # print(b_errors)\n",
    "\n",
    "\n",
    "            # gradient descent \n",
    "            for l in range(0, self.hidden_layer_n+1):\n",
    "                self.weights[l] -= η / minibatch_pool * a_errors[l]\n",
    "                self.bias[l] -= η / minibatch_pool * b_errors[l]\n",
    "            \n",
    "\n",
    "\n",
    "    def predict(self, \n",
    "                input: list[np.array]) -> list[np.array]:\n",
    "        \n",
    "        \"\"\"\n",
    "    Predicts the output for a given input using the trained neural network.\n",
    "\n",
    "    Parameters:\n",
    "        input (np.array): Input data to predict, where each row corresponds to a single input instance.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predictions where each prediction corresponds to the output of the neural network\n",
    "              for the corresponding input instance.\n",
    "\n",
    "    Description:\n",
    "        - Performs forward propagation through the network to compute the output layer activations.\n",
    "        - Returns the final layer activations as predictions.\n",
    "\n",
    "    Example:\n",
    "        predictions = nn.predict(x_test)\n",
    "        \"\"\"\n",
    "        \n",
    "        results = [ ]\n",
    "        for x in input : \n",
    "            # print(x,\"\\n\")\n",
    "            z0 = self.activation_f(self.weights[0] @ x + self.bias[0])\n",
    "            activations = [z0]\n",
    "            for l in range(1, self.layer_n-1):\n",
    "                zl = self.weights[l] @ activations[l-1] + self.bias[l]\n",
    "                a = self.activation_f(zl)\n",
    "                activations.append(a) \n",
    "        \n",
    "            results.append(activations[-1])\n",
    "\n",
    "        return results\n",
    "    \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir('..')\n",
    "import src.neural_network as nn \n",
    "\n",
    "\n",
    "def l2_hinge_loss_gradient(activations, one_hot_label):\n",
    "    \"\"\"\n",
    "    Computes the gradient of L2 multi-class hinge loss for a single instance.\n",
    "\n",
    "    Args:\n",
    "        activations (np.ndarray): Activations (logits) for a single instance (shape: (C,)).\n",
    "        one_hot_label (np.ndarray): One-hot encoded label (shape: (C,)).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Gradient of the L2 hinge loss with respect to activations (shape: (C,)).\n",
    "    \"\"\"\n",
    "    # Find the index of the correct class\n",
    "    y_index = np.argmax(one_hot_label)\n",
    "    \n",
    "    # Initialize gradient\n",
    "    grad = np.zeros_like(activations)\n",
    "    \n",
    "    # Compute margin for each class\n",
    "    for j in range(len(activations)):\n",
    "        if j == y_index:\n",
    "            continue  # Skip the correct class for now\n",
    "        \n",
    "        margin = 1 + activations[j] - activations[y_index]\n",
    "        if margin > 0:\n",
    "            grad[j] = 2 * margin\n",
    "            grad[y_index] -= 2 * margin  # Accumulate gradient for the correct class\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def step(x:np.array,x2: (int | float)=.5) -> np.array:\n",
    "    \"\"\"\n",
    "    Renames the piecewise function htanh(x).\n",
    "    \n",
    "    Args:\n",
    "        x: A scalar or NumPy array.\n",
    "        \n",
    "    Returns:\n",
    "        A NumPy array with the heaviside transformation applied element-wise.\n",
    "    \"\"\"\n",
    "    return np.heaviside(x,x2)\n",
    "\n",
    "def htanh(x):\n",
    "    \"\"\"\n",
    "    Implements the piecewise function htanh(x).\n",
    "    \n",
    "    Args:\n",
    "        x: A scalar or NumPy array.\n",
    "        \n",
    "    Returns:\n",
    "        A NumPy array with the htanh transformation applied element-wise.\n",
    "    \"\"\"\n",
    "    return np.where(x < -1, -1, np.where(x > 1, 1, x))\n",
    "\n",
    "\n",
    "def derivative_htanh(x):\n",
    "    \"\"\"\n",
    "    Implements the derivative of piecewise function htanh(x).\n",
    "    \n",
    "    Args:\n",
    "        x: A scalar or NumPy array.\n",
    "        \n",
    "    Returns:\n",
    "        A NumPy array with the htanh transformation applied element-wise.\n",
    "    \"\"\"\n",
    "    return np.where(x < -1, 0, np.where(x > 1, 0, 1))\n",
    "\n",
    "class NeuralNetworkwMemory(NeuralNetwork):\n",
    "    def train(self,\n",
    "              input, \n",
    "              output, \n",
    "              minibatch: bool = True,\n",
    "              minibatch_pool : (int | float) = 10,\n",
    "              iterations: (int | float) = 100,\n",
    "              η: (int | float) = 1e-6) -> list[np.array]:\n",
    "        \"\"\"\n",
    "        Trains the neural network using gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "            minibatch (bool): Whether to use mini-batch gradient descent (default: True).\n",
    "            minibatch_pool (int | float): Size of the mini-batch for training (default: 10).\n",
    "            iterations (int | float): Number of training iterations (default: 100).\n",
    "            η (int | float): Learning rate for gradient descent (default: 1e-6).\n",
    "\n",
    "        Returns:\n",
    "            NeuralNetwork: The trained neural network object.\n",
    "\n",
    "        Description:\n",
    "            - Implements forward propagation for each input to compute activations.\n",
    "            - Performs backpropagation to compute gradients for weights and biases.\n",
    "            - Updates weights and biases using gradient descent.\n",
    "            - Supports mini-batch gradient descent if `minibatch` is set to True.\n",
    "\n",
    "        Example:\n",
    "            nn.train(minibatch=True, minibatch_pool=32, iterations=1000, η=0.01)\n",
    "        \"\"\"\n",
    "        \n",
    "        weight_history = []\n",
    "\n",
    "        for _ in range(iterations):\n",
    "\n",
    "            if minibatch:\n",
    "                indexes = np.random.choice(input.shape[0], size=minibatch_pool)\n",
    "                X, Y = input[indexes], output[indexes]\n",
    "            else: \n",
    "                X, Y = input, output\n",
    "\n",
    "\n",
    "            a_errors = [np.zeros(matrix.shape) for matrix in self.weights]\n",
    "\n",
    "            b_errors = [np.zeros(vector.shape) for vector in self.bias]\n",
    "\n",
    "            # iterate for each set of x and y\n",
    "            # find zs and as (pre-act and activation)\n",
    "            for x, y in zip(X, Y): \n",
    "                # print(\"x id:\",id(x))\n",
    "                z0 = self.weights[0] @ x + self.bias[0]\n",
    "                zs = [z0]\n",
    "                a0 = self.activation_f(z0)\n",
    "                activations = [a0]\n",
    "                for l in range(1, self.layer_n-1,1):\n",
    "                    # print(\"layers:\",l,l-1)\n",
    "                    zl = self.weights[l] @ activations[l-1] + self.bias[l]\n",
    "                    activation = self.activation_f(zl)\n",
    "                    # print(\"activation:\", activation)\n",
    "                    zs.append(zl)\n",
    "                    activations.append(activation)  \n",
    "\n",
    "                z_output = zs[-1]\n",
    "                a_output = activations[-1]\n",
    "                output_error = self.cost_grad(a_output,y) * self.activation_df(z_output)\n",
    "                errors = [output_error]\n",
    "                for l in range(self.hidden_layer_n, 0, -1):\n",
    "                    error = self.weights[l].T @ errors[-1] * self.activation_df(zs[l-1])\n",
    "                    errors.append(error)\n",
    "\n",
    "                errors.reverse()\n",
    "                # compute sum of error\n",
    "                for l in range(0,self.hidden_layer_n+1,1):\n",
    "                    # a_errors[l] += errors[l]@activations[l].T\n",
    "                    a_errors[l] += np.outer(errors[l], activations[l-1]  if l > 0 else x)\n",
    "                    # print(a_errors)\n",
    "                    b_errors[l] += errors[l] \n",
    "                        # print(b_errors)\n",
    "\n",
    "\n",
    "            # gradient descent \n",
    "            for l in range(0, self.hidden_layer_n+1):\n",
    "                self.weights[l] -= η / minibatch_pool * a_errors[l]\n",
    "                self.bias[l] -= η / minibatch_pool * b_errors[l]\n",
    "            \n",
    "\n",
    "            weight_history.append(self.weights)\n",
    "\n",
    "        return weight_history\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = nn.prepare_data()\n",
    "\n",
    "x_train, x_test = np.round(x_train), np.round(x_test)\n",
    "\n",
    "n_x, n_y = 784, 10  \n",
    "\n",
    "net = NeuralNetworkwMemory([n_x,\n",
    "                         512, \n",
    "                         512, \n",
    "                         512, \n",
    "                         n_y],\n",
    "                       activation_function=step,\n",
    "                       activation_derivative=derivative_htanh,\n",
    "                       cost_grad=l2_hinge_loss_gradient)\n",
    "\n",
    "\n",
    "validation_rate = []\n",
    "for run in range(0, 1000):\n",
    "    net.train(x_train, y_train, minibatch_pool=32, iterations=5, η=.3)\n",
    "    results = net.predict(x_test)\n",
    "    rate = np.sum(\n",
    "            np.array(list(map(np.argmax, results)))\n",
    "            == np.array(list(map(np.argmax, y_test)))\n",
    "        )\n",
    "    validation_rate.append(rate / len(x_test) * 100)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(validation_rate, label=r\"η=3\", color=\"pink\")\n",
    "plt.title(r\"Training: Epoch vrs. Validation Rate\")\n",
    "plt.xlabel(\"epoch (5 x each)\")\n",
    "plt.ylabel(\"Validation Rate (%)\")\n",
    "ticks = np.arange(0, 101, 10)\n",
    "plt.xticks(ticks, labels=ticks.astype(str), minor=True)\n",
    "plt.yticks(ticks, labels=ticks.astype(str), minor=True)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form triadigonal M matrix \n",
    "\n",
    "\n",
    "net = NeuralNetworkwMemory([n_x,\n",
    "                         512, \n",
    "                         512, \n",
    "                         512, \n",
    "                         n_y],\n",
    "                       activation_function=step,\n",
    "                       activation_derivative=derivative_htanh,\n",
    "                       cost_grad=l2_hinge_loss_gradient)\n",
    "\n",
    "weight_history = net.train(x_train, y_train, minibatch_pool=64, iterations=960, η=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(net.weights)\n",
    "\n",
    "def make_R(layer):\n",
    "    return 0 \n",
    "\n",
    "M = [[0 for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "for i in range(0,n): \n",
    "    for j in range(0,n):\n",
    "        if i == j :\n",
    "           R = make_R(i) \n",
    "           M[i][j] = R\n",
    "        else: \n",
    "           M[i,j] = weight_history[i-1].T\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 2. 0. 0.]\n",
      " [0. 0. 3. 4. 0. 0.]\n",
      " [1. 2. 0. 0. 5. 6.]\n",
      " [3. 4. 0. 0. 7. 8.]\n",
      " [0. 0. 5. 6. 0. 0.]\n",
      " [0. 0. 7. 8. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def block_tridiagonal(blocks):\n",
    "    \"\"\"\n",
    "    Create a block tridiagonal matrix with zero blocks on the diagonal.\n",
    "    \n",
    "    Parameters:\n",
    "        blocks (list of numpy arrays): A list of 2D NumPy arrays representing the off-diagonal blocks.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The resulting block tridiagonal matrix.\n",
    "    \"\"\"\n",
    "    num_blocks = len(blocks)\n",
    "    block_size = blocks[0].shape[0]\n",
    "    total_size = num_blocks * block_size\n",
    "\n",
    "    # Initialize the full block matrix with zeros\n",
    "    block_matrix = np.zeros((total_size, total_size))\n",
    "\n",
    "    # Place the off-diagonal blocks\n",
    "    for i in range(num_blocks - 1):\n",
    "        block_matrix[i * block_size:(i + 1) * block_size, (i + 1) * block_size:(i + 2) * block_size] = blocks[i]\n",
    "        block_matrix[(i + 1) * block_size:(i + 2) * block_size, i * block_size:(i + 1) * block_size] = blocks[i]\n",
    "\n",
    "    return block_matrix\n",
    "\n",
    "# Example usage\n",
    "M1 = np.array([[1, 2], [3, 4]])\n",
    "M2 = np.array([[5, 6], [7, 8]])\n",
    "M3 = np.array([[9, 10], [11, 12]])\n",
    "\n",
    "# List of off-diagonal blocks\n",
    "blocks = [M1, M2, M3]\n",
    "\n",
    "# Generate the block tridiagonal matrix\n",
    "result = block_tridiagonal(blocks)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Block 0 row size (784) does not match Block 1 row size (512).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m blocks \u001b[38;5;241m=\u001b[39m [M1, M2, M3]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Generate the block tridiagonal matrix\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mblock_tridiagonal_with_transpose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_history\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Print the result\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[92], line 19\u001b[0m, in \u001b[0;36mblock_tridiagonal_with_transpose\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m blocks[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m blocks[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlock \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m row size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblocks[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlock \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m row size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblocks[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate the total size of the block matrix\u001b[39;00m\n\u001b[1;32m     23\u001b[0m total_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(block\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m blocks)\n",
      "\u001b[0;31mValueError\u001b[0m: Block 0 row size (784) does not match Block 1 row size (512)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "#TODO: generalize this method\n",
    "\n",
    "def block_tridiagonal_with_transpose(blocks):\n",
    "    \"\"\"\n",
    "    Create a block tridiagonal matrix with zero blocks on the diagonal and \n",
    "    each block transposed in the row below where it first appears, supporting varying block sizes.\n",
    "    \n",
    "    Parameters:\n",
    "        blocks (list of numpy arrays): A list of 2D NumPy arrays representing the off-diagonal blocks.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The resulting block tridiagonal matrix.\n",
    "    \"\"\"\n",
    "    # Validate that blocks have consistent row and column relationships\n",
    "    for i in range(len(blocks) - 1):\n",
    "        if blocks[i].shape[1] != blocks[i + 1].shape[1]:\n",
    "            raise ValueError(f\"Block {i} column size ({blocks[i].shape[1]}) does not match \"\n",
    "                             f\"Block {i+1} row size ({blocks[i+1].shape[0]}).\")\n",
    "\n",
    "    # Calculate the total size of the block matrix\n",
    "    total_rows = sum(block.shape[0] for block in blocks)\n",
    "    total_cols = total_rows  # Square block matrix\n",
    "\n",
    "    # Initialize the block matrix with zeros\n",
    "    block_matrix = np.zeros((total_rows, total_cols))\n",
    "\n",
    "    # Place off-diagonal blocks and their transposes\n",
    "    current_row = 0\n",
    "    for i, block in enumerate(blocks):\n",
    "        next_row = current_row + block.shape[0]\n",
    "        next_col = current_row + block.shape[1]\n",
    "\n",
    "        if i < len(blocks) - 1:\n",
    "            # Place block in the upper off-diagonal\n",
    "            block_matrix[current_row:next_row, next_row:next_row + block.shape[1]] = block\n",
    "            # Place the transposed block in the lower off-diagonal\n",
    "            block_matrix[next_row:next_row + block.shape[1], current_row:next_row] = block.T\n",
    "\n",
    "        current_row += block.shape[0]\n",
    "\n",
    "    return block_matrix\n",
    "# Example usage\n",
    "M1 = np.array([[1, 2], [3, 4]])\n",
    "M2 = np.array([[5, 6], [7, 8]])\n",
    "M3 = np.array([[9, 10], [11, 12]])\n",
    "\n",
    "# List of off-diagonal blocks\n",
    "blocks = [M1, M2, M3]\n",
    "\n",
    "# Generate the block tridiagonal matrix\n",
    "result = block_tridiagonal_with_transpose(weight_history[0])\n",
    "\n",
    "# Print the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 784)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def form_R(β:(int | float ), weights:list[np.array]) -> list[np.array]:\n",
    "    Rs = []\n",
    "    L = len(weights)\n",
    "    for l in range(0,L): \n",
    "        if l == 0 :\n",
    "            R0 = []\n",
    "            (m, _) , J0 = weights[0].shape, weights[0]\n",
    "            for i in range(0,m):\n",
    "                R0.append(-β* sum(J0[i,:]**2))\n",
    "        if  0 < l < L-1:\n",
    "            Rk = []\n",
    "            (m, _) , JL1,JL = weights[l-1].shape, weights[l], weights[l-1]\n",
    "            for i in range(0,m):\n",
    "                row = [ ]\n",
    "                for j in range(0,n):\n",
    "                    if i != j :\n",
    "                        row.append(0)\n",
    "                    if i == j :\n",
    "                        row.append(-β* ( sum(JL[:,i]**2) + sum(JL1[i]**2)) )\n",
    "                Rk.append(row)\n",
    "            Rs.append(np.array(Rk).T)\n",
    "\n",
    "        else: \n",
    "            RL = []\n",
    "            (m, n) , JL = weights[L-1].shape, weights[L-1]\n",
    "            for i in range(0,m):\n",
    "                row = [ ]\n",
    "                for j in range(0,n):\n",
    "                    if i != j :\n",
    "                        row.append(0)\n",
    "                    if i == j :\n",
    "                        row.append(-β* sum(JL[:, i]**2))\n",
    "                RL.append(row)\n",
    "            Rs.append(np.array(RL).T)\n",
    "\n",
    "    return Rs\n",
    "\n",
    "\n",
    "\n",
    "Rs=form_R(β=2, weights=weight_history[0])\n",
    "\n",
    "J = weight_history[0]\n",
    "\n",
    "\n",
    "z1 = np.zeros(Rs[0].shape)\n",
    "z2 = np.zeros(J[1].shape)\n",
    "z3 = np.zeros(J[2].shape)\n",
    "M = [ [Rs[0], J[0],0,0,0], \n",
    "        [J[0].T, Rs[1],J[1],0,0],\n",
    "        [0,J[1].T, Rs[2],J[2],0],\n",
    "        [0,0,J[2].T,Rs[3],J[3]],\n",
    "        [0,0,0,J[3].T, Rs[4]]]\n",
    "\n",
    "# def form_M(weights):\n",
    "\n",
    "\n",
    "J[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 784)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01870085,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        , -0.01419157,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , -0.01405076, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β = 1 \n",
    "RL = []\n",
    "(m, n) , JL = weight_history[0][3].shape, weight_history[0][3]\n",
    "for i in range(0,m):\n",
    "    row = [ ]\n",
    "    for j in range(0,n):\n",
    "        if i != j :\n",
    "            row.append(0)\n",
    "\n",
    "        if i == j :\n",
    "\n",
    "            row.append(-β* sum(JL[:, j]**2))\n",
    "                  \n",
    "    RL.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.62648788,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        , -1.56493599,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , -1.69095368, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -1.70394971,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        -1.64436159,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        , -1.5908167 ]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rk = []\n",
    "\n",
    "(m, n) , JL1,JL = weight_history[0][1].shape, weight_history[0][1], weight_history[0][0]\n",
    "for i in range(0,m):\n",
    "    row = [ ]\n",
    "    for j in range(0,n):\n",
    "        if i != j :\n",
    "            row.append(0)\n",
    "        if i == j :\n",
    "            row.append(-β* ( sum(JL[:,i]**2) + sum(JL1[i]**2)) )\n",
    "                  \n",
    "    Rk.append(row)\n",
    "\n",
    "np.array(Rk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mform_R\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_history\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[139], line 32\u001b[0m, in \u001b[0;36mform_R\u001b[0;34m(β, weights)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m j :\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 32\u001b[0m             row\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39mβ\u001b[38;5;241m*\u001b[39m ( \u001b[38;5;28msum\u001b[39m(\u001b[43mJL\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(JL1[i]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)) )\n\u001b[1;32m     34\u001b[0m     Rk\u001b[38;5;241m.\u001b[39mappend(row)\n\u001b[1;32m     36\u001b[0m Rs\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(Rk))\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "form_R(1, weight_history[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = weight_history[0]\n",
    "R0 = []\n",
    "(m, _) , J0 = weights[0].shape, weights[0]\n",
    "for i in range(0,m):\n",
    "    R0.append(-β* sum(J0[i,:]**2))\n",
    "\n",
    "np.diag(R0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mR0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "R0[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 784)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 783 is out of bounds for axis 0 with size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[235], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Jl_1))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,m):\n\u001b[0;32m---> 14\u001b[0m     Rk2\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28msum\u001b[39m(\u001b[43mJl_1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)) )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mlen\u001b[39m(Jl_1)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 783 is out of bounds for axis 0 with size 512"
     ]
    }
   ],
   "source": [
    "l=1\n",
    "Rk1 = []\n",
    "Rk2 = []\n",
    "(_, n) , (m,_),Jl_1,Jl = weights[l-1].shape, weights[l].shape, weights[l], weights[l-1]\n",
    "\n",
    "\n",
    "for i in range(0,n):\n",
    "    Rk1.append((sum(Jl[:,i]**2)) )\n",
    "\n",
    "\n",
    "print(len(Jl_1))\n",
    "for j in range(0,m):\n",
    "\n",
    "    Rk2.append((sum(Jl_1[i,:]**2)) )\n",
    "\n",
    "\n",
    "len(Jl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 784)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearized_TAP(J, m, β ):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    D = np.diag(1-m**2)\n",
    "    d = np.diag(J**2*(1-m**2))\n",
    "    return 1 - D @ J + β*D @ d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 512 is different from 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[239], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlinearized_TAP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_history\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[238], line 8\u001b[0m, in \u001b[0;36mlinearized_TAP\u001b[0;34m(J, m, β)\u001b[0m\n\u001b[1;32m      6\u001b[0m D \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mm\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(J\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mm\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mD\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mJ\u001b[49m \u001b[38;5;241m+\u001b[39m β\u001b[38;5;241m*\u001b[39mD \u001b[38;5;241m@\u001b[39m d\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 512 is different from 784)"
     ]
    }
   ],
   "source": [
    "linearized_TAP(weight_history[0][0],x_train[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "J = weight_history[0][0]\n",
    "m = x_train[0]\n",
    "D = np.diag(1-m**2)\n",
    "d = np.diag(J**2 @ (1-m**2))\n",
    "\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 784)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 512)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_history[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2832"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784+512+512+512+10\n",
    "\n",
    "784+512*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = weight_history[0]\n",
    "o_00 = np.zeros((784,784))\n",
    "o_02 = np.zeros((784,512))\n",
    "o_03 = np.zeros((784,10))\n",
    "o_10 = np.zeros((512,512))\n",
    "o_11 = np.zeros((512,10))\n",
    "o_12 = np.zeros((10,10))\n",
    "\n",
    "\n",
    "J = [\n",
    "          \n",
    "          [o_00,        W[0].T,      o_02,       o_02,       o_03 ] ,\n",
    "          [ W[0],       o_10,        W[1].T,     o_10,       o_11]  ,\n",
    "          [o_02.T,      W[1],        o_10,       W[2].T,     o_11 ] ,\n",
    "          [o_02.T,      o_10,        W[2],       o_10,       W[3].T] , \n",
    "          [o_03.T,      o_11.T,      o_11.T,     W[3],       o_12 ]\n",
    "          \n",
    "          ]\n",
    "\n",
    "# m = n = 2330\n",
    "J = np.block(J)\n",
    "\n",
    "\n",
    "def form_M(J, β, inplace=False):\n",
    "    if inplace is False: \n",
    "        J_  = J.copy()\n",
    "    else: \n",
    "        J_ = J \n",
    "    J_ = β * J_\n",
    "\n",
    "        # Add the diagonal correction\n",
    "    diagonal_correction = β**2 * np.sum(J_**2, axis=1)\n",
    "    np.fill_diagonal(J_, np.diagonal(J_) + diagonal_correction)\n",
    "    return J_\n",
    "\n",
    "M = form_M(J, β=1)\n",
    "\n",
    "eig_M = np.linalg.eig(1-M)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.329e+03, 1.000e+00]),\n",
       " array([  -4.38463153, 1162.12574258, 2328.63611669]),\n",
       " <BarContainer object of 2 artists>)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIKRJREFUeJzt3X1wVNXBx/FfEtgFhN3wlmyiAYMvvAmIQcOOwNSSSYKRaqEzgKmijTDSxBkMItLagG2nWGi1ahHqtBo74wsyU7BCjcZgSMUQNJoCETNgYwOFTRRMNkFIAjnPHx3u4xa0BjdsTvx+Zu4M2XP27rl3Jfm6ubtEGWOMAAAALBId6QUAAAB0FgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDq9Ir2ArtLR0aHDhw9rwIABioqKivRyAADA12CMUXNzsxITExUd/eWvs/TYgDl8+LCSkpIivQwAAHAeDh48qEsuueRLx3tswAwYMEDSf06Ax+OJ8GoAAMDXEQwGlZSU5Pwc/zI9NmDO/NrI4/EQMAAAWOZ/Xf7BRbwAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALBOr0gvwEaXPrA10ksAvvU+fjgr0ksAEEG8AgMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACs06mAWbVqla699loNGDBAcXFxuuWWW1RTUxMy5+TJk8rNzdXgwYPVv39/zZ49W/X19SFz6urqlJWVpX79+ikuLk5Lly7VqVOnQuaUlpbqmmuukdvt1uWXX67CwsLzO0IAANDjdCpgtm/frtzcXO3cuVPFxcVqb29Xenq6jh8/7sy599579corr2jjxo3avn27Dh8+rFmzZjnjp0+fVlZWltra2vT222/r2WefVWFhoQoKCpw5tbW1ysrK0g033KCqqiotXrxYd911l1577bUwHDIAALBdlDHGnO+dP/nkE8XFxWn79u2aNm2ampqaNHToUD3//PP6wQ9+IEn68MMPNXr0aJWXl2vy5Ml69dVXddNNN+nw4cOKj4+XJK1fv17Lli3TJ598IpfLpWXLlmnr1q3au3ev81hz585VY2OjioqKvtbagsGgvF6vmpqa5PF4zvcQz+nSB7aGdX8AOu/jh7MivQQAXeDr/vz+RtfANDU1SZIGDRokSaqsrFR7e7vS0tKcOaNGjdKwYcNUXl4uSSovL9e4ceOceJGkjIwMBYNBVVdXO3O+uI8zc87s41xaW1sVDAZDNgAA0DOdd8B0dHRo8eLFuv7663XVVVdJkgKBgFwul2JjY0PmxsfHKxAIOHO+GC9nxs+MfdWcYDCoEydOnHM9q1atktfrdbakpKTzPTQAANDNnXfA5Obmau/evXrxxRfDuZ7ztnz5cjU1NTnbwYMHI70kAADQRXqdz53y8vK0ZcsWlZWV6ZJLLnFu9/l8amtrU2NjY8irMPX19fL5fM6cXbt2hezvzLuUvjjnv9+5VF9fL4/Ho759+55zTW63W263+3wOBwAAWKZTr8AYY5SXl6dNmzZp27ZtSk5ODhlPSUlR7969VVJS4txWU1Ojuro6+f1+SZLf79eePXvU0NDgzCkuLpbH49GYMWOcOV/cx5k5Z/YBAAC+3Tr1Ckxubq6ef/55vfzyyxowYIBzzYrX61Xfvn3l9XqVk5Oj/Px8DRo0SB6PR/fcc4/8fr8mT54sSUpPT9eYMWN02223afXq1QoEAnrwwQeVm5vrvIJy99136/e//73uv/9+/ehHP9K2bdv00ksvaetW3v0DAAA6+QrMunXr1NTUpO985ztKSEhwtg0bNjhzHn30Ud10002aPXu2pk2bJp/Pp7/85S/OeExMjLZs2aKYmBj5/X798Ic/1O23366f//znzpzk5GRt3bpVxcXFmjBhgn7729/qj3/8ozIyMsJwyAAAwHbf6HNgujM+Bwbo2fgcGKBnuiCfAwMAABAJBAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOt0OmDKyso0c+ZMJSYmKioqSps3bw4Zv+OOOxQVFRWyZWZmhsw5duyYsrOz5fF4FBsbq5ycHLW0tITM2b17t6ZOnao+ffooKSlJq1ev7vzRAQCAHqnTAXP8+HFNmDBBa9eu/dI5mZmZOnLkiLO98MILIePZ2dmqrq5WcXGxtmzZorKyMi1cuNAZDwaDSk9P1/Dhw1VZWak1a9Zo5cqVeuqppzq7XAAA0AP16uwdZsyYoRkzZnzlHLfbLZ/Pd86xffv2qaioSO+8844mTZokSXriiSd044036je/+Y0SExP13HPPqa2tTU8//bRcLpfGjh2rqqoqPfLIIyGhAwAAvp265BqY0tJSxcXFaeTIkVq0aJGOHj3qjJWXlys2NtaJF0lKS0tTdHS0KioqnDnTpk2Ty+Vy5mRkZKimpkafffZZVywZAABYpNOvwPwvmZmZmjVrlpKTk/XRRx/pJz/5iWbMmKHy8nLFxMQoEAgoLi4udBG9emnQoEEKBAKSpEAgoOTk5JA58fHxztjAgQPPetzW1la1trY6XweDwXAfGgAA6CbCHjBz5851/jxu3DiNHz9el112mUpLSzV9+vRwP5xj1apVeuihh7ps/wAAoPvo8rdRjxgxQkOGDNGBAwckST6fTw0NDSFzTp06pWPHjjnXzfh8PtXX14fMOfP1l11bs3z5cjU1NTnbwYMHw30oAACgm+jygDl06JCOHj2qhIQESZLf71djY6MqKyudOdu2bVNHR4dSU1OdOWVlZWpvb3fmFBcXa+TIkef89ZH0nwuHPR5PyAYAAHqmTgdMS0uLqqqqVFVVJUmqra1VVVWV6urq1NLSoqVLl2rnzp36+OOPVVJSoptvvlmXX365MjIyJEmjR49WZmamFixYoF27dmnHjh3Ky8vT3LlzlZiYKEm69dZb5XK5lJOTo+rqam3YsEGPPfaY8vPzw3fkAADAWp0OmHfffVcTJ07UxIkTJUn5+fmaOHGiCgoKFBMTo927d+t73/uerrzySuXk5CglJUV///vf5Xa7nX0899xzGjVqlKZPn64bb7xRU6ZMCfmMF6/Xq9dff121tbVKSUnRkiVLVFBQwFuoAQCAJCnKGGMivYiuEAwG5fV61dTUFPZfJ136wNaw7g9A5338cFaklwCgC3zdn9/8W0gAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDqdDpiysjLNnDlTiYmJioqK0ubNm0PGjTEqKChQQkKC+vbtq7S0NO3fvz9kzrFjx5SdnS2Px6PY2Fjl5OSopaUlZM7u3bs1depU9enTR0lJSVq9enXnjw4AAPRInQ6Y48ePa8KECVq7du05x1evXq3HH39c69evV0VFhS666CJlZGTo5MmTzpzs7GxVV1eruLhYW7ZsUVlZmRYuXOiMB4NBpaena/jw4aqsrNSaNWu0cuVKPfXUU+dxiAAAoKeJMsaY875zVJQ2bdqkW265RdJ/Xn1JTEzUkiVLdN9990mSmpqaFB8fr8LCQs2dO1f79u3TmDFj9M4772jSpEmSpKKiIt144406dOiQEhMTtW7dOv30pz9VIBCQy+WSJD3wwAPavHmzPvzww6+1tmAwKK/Xq6amJnk8nvM9xHO69IGtYd0fgM77+OGsSC8BQBf4uj+/w3oNTG1trQKBgNLS0pzbvF6vUlNTVV5eLkkqLy9XbGysEy+SlJaWpujoaFVUVDhzpk2b5sSLJGVkZKimpkafffbZOR+7tbVVwWAwZAMAAD1TWAMmEAhIkuLj40Nuj4+Pd8YCgYDi4uJCxnv16qVBgwaFzDnXPr74GP9t1apV8nq9zpaUlPTNDwgAAHRLPeZdSMuXL1dTU5OzHTx4MNJLAgAAXSSsAePz+SRJ9fX1IbfX19c7Yz6fTw0NDSHjp06d0rFjx0LmnGsfX3yM/+Z2u+XxeEI2AADQM4U1YJKTk+Xz+VRSUuLcFgwGVVFRIb/fL0ny+/1qbGxUZWWlM2fbtm3q6OhQamqqM6esrEzt7e3OnOLiYo0cOVIDBw4M55IBAICFOh0wLS0tqqqqUlVVlaT/XLhbVVWluro6RUVFafHixfrlL3+pv/71r9qzZ49uv/12JSYmOu9UGj16tDIzM7VgwQLt2rVLO3bsUF5enubOnavExERJ0q233iqXy6WcnBxVV1drw4YNeuyxx5Sfnx+2AwcAAPbq1dk7vPvuu7rhhhucr89Exfz581VYWKj7779fx48f18KFC9XY2KgpU6aoqKhIffr0ce7z3HPPKS8vT9OnT1d0dLRmz56txx9/3Bn3er16/fXXlZubq5SUFA0ZMkQFBQUhnxUDAAC+vb7R58B0Z3wODNCz8TkwQM8Ukc+BAQAAuBAIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdcIeMCtXrlRUVFTINmrUKGf85MmTys3N1eDBg9W/f3/Nnj1b9fX1Ifuoq6tTVlaW+vXrp7i4OC1dulSnTp0K91IBAIClenXFTseOHas33njj/x+k1/8/zL333qutW7dq48aN8nq9ysvL06xZs7Rjxw5J0unTp5WVlSWfz6e3335bR44c0e23367evXvrV7/6VVcsFwAAWKZLAqZXr17y+Xxn3d7U1KQ//elPev755/Xd735XkvTMM89o9OjR2rlzpyZPnqzXX39dH3zwgd544w3Fx8fr6quv1i9+8QstW7ZMK1eulMvl6oolAwAAi3TJNTD79+9XYmKiRowYoezsbNXV1UmSKisr1d7errS0NGfuqFGjNGzYMJWXl0uSysvLNW7cOMXHxztzMjIyFAwGVV1d/aWP2draqmAwGLIBAICeKewBk5qaqsLCQhUVFWndunWqra3V1KlT1dzcrEAgIJfLpdjY2JD7xMfHKxAISJICgUBIvJwZPzP2ZVatWiWv1+tsSUlJ4T0wAADQbYT9V0gzZsxw/jx+/HilpqZq+PDheumll9S3b99wP5xj+fLlys/Pd74OBoNEDAAAPVSXv406NjZWV155pQ4cOCCfz6e2tjY1NjaGzKmvr3eumfH5fGe9K+nM1+e6ruYMt9stj8cTsgEAgJ6pywOmpaVFH330kRISEpSSkqLevXurpKTEGa+pqVFdXZ38fr8kye/3a8+ePWpoaHDmFBcXy+PxaMyYMV29XAAAYIGw/wrpvvvu08yZMzV8+HAdPnxYK1asUExMjObNmyev16ucnBzl5+dr0KBB8ng8uueee+T3+zV58mRJUnp6usaMGaPbbrtNq1evViAQ0IMPPqjc3Fy53e5wLxcAAFgo7AFz6NAhzZs3T0ePHtXQoUM1ZcoU7dy5U0OHDpUkPfroo4qOjtbs2bPV2tqqjIwMPfnkk879Y2JitGXLFi1atEh+v18XXXSR5s+fr5///OfhXioAALBUlDHGRHoRXSEYDMrr9aqpqSns18Nc+sDWsO4PQOd9/HBWpJcAoAt83Z/f/FtIAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrdOuAWbt2rS699FL16dNHqamp2rVrV6SXBAAAuoFuGzAbNmxQfn6+VqxYoffee08TJkxQRkaGGhoaIr00AAAQYd02YB555BEtWLBAd955p8aMGaP169erX79+evrppyO9NAAAEGG9Ir2Ac2lra1NlZaWWL1/u3BYdHa20tDSVl5ef8z6tra1qbW11vm5qapIkBYPBsK+vo/XzsO8TQOd0xd9tAJF35u+2MeYr53XLgPn00091+vRpxcfHh9weHx+vDz/88Jz3WbVqlR566KGzbk9KSuqSNQKILO/vIr0CAF2publZXq/3S8e7ZcCcj+XLlys/P9/5uqOjQ8eOHdPgwYMVFRUV1scKBoNKSkrSwYMH5fF4wrpvfD08B5HHcxBZnP/I4znoGsYYNTc3KzEx8SvndcuAGTJkiGJiYlRfXx9ye319vXw+3znv43a75Xa7Q26LjY3tqiVKkjweD//RRhjPQeTxHEQW5z/yeA7C76teeTmjW17E63K5lJKSopKSEue2jo4OlZSUyO/3R3BlAACgO+iWr8BIUn5+vubPn69Jkybpuuuu0+9+9zsdP35cd955Z6SXBgAAIqzbBsycOXP0ySefqKCgQIFAQFdffbWKiorOurA3Etxut1asWHHWr6xw4fAcRB7PQWRx/iOP5yCyosz/ep8SAABAN9Mtr4EBAAD4KgQMAACwDgEDAACsQ8AAAADrEDDnYe3atbr00kvVp08fpaamateuXZFeUo+wcuVKRUVFhWyjRo1yxk+ePKnc3FwNHjxY/fv31+zZs8/6sMO6ujplZWWpX79+iouL09KlS3Xq1KkLfSjWKCsr08yZM5WYmKioqCht3rw5ZNwYo4KCAiUkJKhv375KS0vT/v37Q+YcO3ZM2dnZ8ng8io2NVU5OjlpaWkLm7N69W1OnTlWfPn2UlJSk1atXd/WhWeF/nf877rjjrL8TmZmZIXM4/+dv1apVuvbaazVgwADFxcXplltuUU1NTciccH3fKS0t1TXXXCO3263LL79chYWFXX14PR4B00kbNmxQfn6+VqxYoffee08TJkxQRkaGGhoaIr20HmHs2LE6cuSIs7311lvO2L333qtXXnlFGzdu1Pbt23X48GHNmjXLGT99+rSysrLU1tamt99+W88++6wKCwtVUFAQiUOxwvHjxzVhwgStXbv2nOOrV6/W448/rvXr16uiokIXXXSRMjIydPLkSWdOdna2qqurVVxcrC1btqisrEwLFy50xoPBoNLT0zV8+HBVVlZqzZo1WrlypZ566qkuP77u7n+df0nKzMwM+TvxwgsvhIxz/s/f9u3blZubq507d6q4uFjt7e1KT0/X8ePHnTnh+L5TW1urrKws3XDDDaqqqtLixYt111136bXXXrugx9vjGHTKddddZ3Jzc52vT58+bRITE82qVasiuKqeYcWKFWbChAnnHGtsbDS9e/c2GzdudG7bt2+fkWTKy8uNMcb87W9/M9HR0SYQCDhz1q1bZzwej2ltbe3StfcEksymTZucrzs6OozP5zNr1qxxbmtsbDRut9u88MILxhhjPvjgAyPJvPPOO86cV1991URFRZl///vfxhhjnnzySTNw4MCQ52DZsmVm5MiRXXxEdvnv82+MMfPnzzc333zzl96H8x9eDQ0NRpLZvn27MSZ833fuv/9+M3bs2JDHmjNnjsnIyOjqQ+rReAWmE9ra2lRZWam0tDTntujoaKWlpam8vDyCK+s59u/fr8TERI0YMULZ2dmqq6uTJFVWVqq9vT3k3I8aNUrDhg1zzn15ebnGjRsX8mGHGRkZCgaDqq6uvrAH0gPU1tYqEAiEnHOv16vU1NSQcx4bG6tJkyY5c9LS0hQdHa2KigpnzrRp0+RyuZw5GRkZqqmp0WeffXaBjsZepaWliouL08iRI7Vo0SIdPXrUGeP8h1dTU5MkadCgQZLC932nvLw8ZB9n5vBz45shYDrh008/1enTp8/6NOD4+HgFAoEIrarnSE1NVWFhoYqKirRu3TrV1tZq6tSpam5uViAQkMvlOusf6PziuQ8EAud8bs6MoXPOnLOv+u89EAgoLi4uZLxXr14aNGgQz0sYZGZm6s9//rNKSkr061//Wtu3b9eMGTN0+vRpSZz/cOro6NDixYt1/fXX66qrrpKksH3f+bI5wWBQJ06c6IrD+Vbotv+UAL59ZsyY4fx5/PjxSk1N1fDhw/XSSy+pb9++EVwZEBlz5851/jxu3DiNHz9el112mUpLSzV9+vQIrqznyc3N1d69e0Ouu0P3xiswnTBkyBDFxMScdQV6fX29fD5fhFbVc8XGxurKK6/UgQMH5PP51NbWpsbGxpA5Xzz3Pp/vnM/NmTF0zplz9lX/vft8vrMuYD916pSOHTvG89IFRowYoSFDhujAgQOSOP/hkpeXpy1btujNN9/UJZdc4tweru87XzbH4/HwP2ffAAHTCS6XSykpKSopKXFu6+joUElJifx+fwRX1jO1tLToo48+UkJCglJSUtS7d++Qc19TU6O6ujrn3Pv9fu3ZsyfkG3pxcbE8Ho/GjBlzwddvu+TkZPl8vpBzHgwGVVFREXLOGxsbVVlZ6czZtm2bOjo6lJqa6swpKytTe3u7M6e4uFgjR47UwIEDL9DR9AyHDh3S0aNHlZCQIInz/00ZY5SXl6dNmzZp27ZtSk5ODhkP1/cdv98fso8zc/i58Q1F+ipi27z44ovG7XabwsJC88EHH5iFCxea2NjYkCvQcX6WLFliSktLTW1trdmxY4dJS0szQ4YMMQ0NDcYYY+6++24zbNgws23bNvPuu+8av99v/H6/c/9Tp06Zq666yqSnp5uqqipTVFRkhg4dapYvXx6pQ+r2mpubzfvvv2/ef/99I8k88sgj5v333zf/+te/jDHGPPzwwyY2Nta8/PLLZvfu3ebmm282ycnJ5sSJE84+MjMzzcSJE01FRYV56623zBVXXGHmzZvnjDc2Npr4+Hhz2223mb1795oXX3zR9OvXz/zhD3+44Mfb3XzV+W9ubjb33XefKS8vN7W1teaNN94w11xzjbniiivMyZMnnX1w/s/fokWLjNfrNaWlpebIkSPO9vnnnztzwvF955///Kfp16+fWbp0qdm3b59Zu3atiYmJMUVFRRf0eHsaAuY8PPHEE2bYsGHG5XKZ6667zuzcuTPSS+oR5syZYxISEozL5TIXX3yxmTNnjjlw4IAzfuLECfPjH//YDBw40PTr1898//vfN0eOHAnZx8cff2xmzJhh+vbta4YMGWKWLFli2tvbL/ShWOPNN980ks7a5s+fb4z5z1upf/azn5n4+HjjdrvN9OnTTU1NTcg+jh49aubNm2f69+9vPB6PufPOO01zc3PInH/84x9mypQpxu12m4svvtg8/PDDF+oQu7WvOv+ff/65SU9PN0OHDjW9e/c2w4cPNwsWLDjrf5Y4/+fvXOdeknnmmWecOeH6vvPmm2+aq6++2rhcLjNixIiQx8D5iTLGmAv9qg8AAMA3wTUwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6/wfz+cApOxxIC4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eig_M.eigenvalues,bins=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[-2.06968777,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        , -1.80358165,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        , -1.99435419, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]]),\n",
       "  array([[-0.00029609,  0.04906848, -0.0575105 , ..., -0.0089513 ,\n",
       "           0.01739463, -0.0244129 ],\n",
       "         [-0.00233965, -0.05187616,  0.05180787, ...,  0.028442  ,\n",
       "          -0.03902454,  0.01658924],\n",
       "         [-0.01488027,  0.0552667 , -0.07325578, ..., -0.03752571,\n",
       "           0.0197217 ,  0.03008509],\n",
       "         ...,\n",
       "         [-0.00219633, -0.06887786, -0.0170154 , ..., -0.04063697,\n",
       "           0.06235516, -0.06760167],\n",
       "         [-0.05184263,  0.02928085, -0.02168367, ...,  0.04385005,\n",
       "           0.01416311, -0.00259156],\n",
       "         [-0.07162024, -0.03018538, -0.0001703 , ..., -0.02382143,\n",
       "           0.00255188,  0.02024766]]),\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [array([[-0.00029609, -0.00233965, -0.01488027, ..., -0.00219633,\n",
       "          -0.05184263, -0.07162024],\n",
       "         [ 0.04906848, -0.05187616,  0.0552667 , ..., -0.06887786,\n",
       "           0.02928085, -0.03018538],\n",
       "         [-0.0575105 ,  0.05180787, -0.07325578, ..., -0.0170154 ,\n",
       "          -0.02168367, -0.0001703 ],\n",
       "         ...,\n",
       "         [-0.0089513 ,  0.028442  , -0.03752571, ..., -0.04063697,\n",
       "           0.04385005, -0.02382143],\n",
       "         [ 0.01739463, -0.03902454,  0.0197217 , ...,  0.06235516,\n",
       "           0.01416311,  0.00255188],\n",
       "         [-0.0244129 ,  0.01658924,  0.03008509, ..., -0.06760167,\n",
       "          -0.00259156,  0.02024766]]),\n",
       "  array([[-0.03740171,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        , -0.02838313,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        , -0.02810152, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]]),\n",
       "  array([[ 0.0252567 ,  0.02999597,  0.02729471, ...,  0.03943541,\n",
       "           0.00703189, -0.02492938],\n",
       "         [ 0.02610829,  0.0725911 , -0.03087502, ..., -0.00286547,\n",
       "          -0.05400916, -0.00915481],\n",
       "         [ 0.09493926,  0.08202724, -0.01596941, ..., -0.0377534 ,\n",
       "          -0.13397363, -0.04922414],\n",
       "         ...,\n",
       "         [ 0.03130919,  0.05068662,  0.06932478, ..., -0.01041212,\n",
       "           0.02177079,  0.02529098],\n",
       "         [ 0.01399502, -0.01081702,  0.04170113, ..., -0.02705927,\n",
       "          -0.0734543 , -0.08102784],\n",
       "         [-0.09321353,  0.06547349, -0.00909195, ..., -0.0263632 ,\n",
       "          -0.10286991, -0.00528563]]),\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  array([[ 0.0252567 ,  0.02610829,  0.09493926, ...,  0.03130919,\n",
       "           0.01399502, -0.09321353],\n",
       "         [ 0.02999597,  0.0725911 ,  0.08202724, ...,  0.05068662,\n",
       "          -0.01081702,  0.06547349],\n",
       "         [ 0.02729471, -0.03087502, -0.01596941, ...,  0.06932478,\n",
       "           0.04170113, -0.00909195],\n",
       "         ...,\n",
       "         [ 0.03943541, -0.00286547, -0.0377534 , ..., -0.01041212,\n",
       "          -0.02705927, -0.0263632 ],\n",
       "         [ 0.00703189, -0.05400916, -0.13397363, ...,  0.02177079,\n",
       "          -0.0734543 , -0.10286991],\n",
       "         [-0.02492938, -0.00915481, -0.04922414, ...,  0.02529098,\n",
       "          -0.08102784, -0.00528563]]),\n",
       "  array([[-3.25297575,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        , -3.12987197,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        , -3.38190736, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ..., -3.40789941,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          -3.28872318,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        , -3.1816334 ]]),\n",
       "  array([[-0.00141029,  0.0649708 ,  0.09811438, ...,  0.10333166,\n",
       "          -0.00505016, -0.08041071],\n",
       "         [-0.0108884 , -0.00433819,  0.00732477, ...,  0.03746677,\n",
       "          -0.10874038, -0.06591072],\n",
       "         [ 0.08427005, -0.02710344,  0.0542498 , ...,  0.01721057,\n",
       "          -0.03614788,  0.0700597 ],\n",
       "         ...,\n",
       "         [ 0.00118084, -0.00798204, -0.01937617, ..., -0.02698149,\n",
       "           0.0583808 ,  0.04779516],\n",
       "         [-0.00302323,  0.01358551, -0.05984057, ..., -0.00847947,\n",
       "           0.02507383,  0.00083238],\n",
       "         [-0.0257506 , -0.06385951, -0.06014494, ..., -0.13827736,\n",
       "           0.03383249,  0.04472181]]),\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  array([[-0.00141029, -0.0108884 ,  0.08427005, ...,  0.00118084,\n",
       "          -0.00302323, -0.0257506 ],\n",
       "         [ 0.0649708 , -0.00433819, -0.02710344, ..., -0.00798204,\n",
       "           0.01358551, -0.06385951],\n",
       "         [ 0.09811438,  0.00732477,  0.0542498 , ..., -0.01937617,\n",
       "          -0.05984057, -0.06014494],\n",
       "         ...,\n",
       "         [ 0.10333166,  0.03746677,  0.01721057, ..., -0.02698149,\n",
       "          -0.00847947, -0.13827736],\n",
       "         [-0.00505016, -0.10874038, -0.03614788, ...,  0.0583808 ,\n",
       "           0.02507383,  0.03383249],\n",
       "         [-0.08041071, -0.06591072,  0.0700597 , ...,  0.04779516,\n",
       "           0.00083238,  0.04472181]]),\n",
       "  array([[-3.85291015,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        , -4.0909293 ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        , -4.24447349, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ..., -4.2830687 ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          -4.12716912,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        , -3.91908314]]),\n",
       "  array([[ 0.03291046,  0.03945004, -0.0176395 , ...,  0.01746608,\n",
       "           0.0287139 ,  0.06333941],\n",
       "         [ 0.06067293,  0.05027146,  0.00617545, ..., -0.01865778,\n",
       "          -0.02310779,  0.11837688],\n",
       "         [-0.05511954,  0.00623683,  0.02504695, ..., -0.02940178,\n",
       "          -0.03317094, -0.02851034],\n",
       "         ...,\n",
       "         [ 0.00255212,  0.01462192, -0.06717118, ...,  0.00932482,\n",
       "          -0.04591461, -0.00586791],\n",
       "         [-0.04133117, -0.05043911,  0.01960917, ..., -0.05844031,\n",
       "           0.00102711,  0.02901841],\n",
       "         [-0.02077686, -0.0587184 ,  0.04280796, ..., -0.00182891,\n",
       "          -0.03879628, -0.06349486]])],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  array([[ 0.03291046,  0.06067293, -0.05511954, ...,  0.00255212,\n",
       "          -0.04133117, -0.02077686],\n",
       "         [ 0.03945004,  0.05027146,  0.00623683, ...,  0.01462192,\n",
       "          -0.05043911, -0.0587184 ],\n",
       "         [-0.0176395 ,  0.00617545,  0.02504695, ..., -0.06717118,\n",
       "           0.01960917,  0.04280796],\n",
       "         ...,\n",
       "         [ 0.01746608, -0.01865778, -0.02940178, ...,  0.00932482,\n",
       "          -0.05844031, -0.00182891],\n",
       "         [ 0.0287139 , -0.02310779, -0.03317094, ..., -0.04591461,\n",
       "           0.00102711, -0.03879628],\n",
       "         [ 0.06333941,  0.11837688, -0.02851034, ..., -0.00586791,\n",
       "           0.02901841, -0.06349486]]),\n",
       "  array([[-0.03740171,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        , -0.02838313,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        , -0.02810152, ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]])]]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".neuralnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
